{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering - Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine grouping your friends based on a their likes, dislikes, interests ,behaviour.\n",
    "\n",
    "Now let us regroup them based on the common characteristics they share.\n",
    "\n",
    "Keep iterating the above steps till you get groups of friends who shares most \n",
    "\n",
    "characteristics.\n",
    "\n",
    "The task we achieved in the above process is termed as clustering.\n",
    "\n",
    "\n",
    "\n",
    "Now , let us understand clustering of data points with the help of this example.\n",
    "\n",
    "Assume your friends to be the data points, the similarity between them as the distance \n",
    "\n",
    "between these data points.If two friends are more similar ,in XY plane it means two data \n",
    "\n",
    "points with lesser distance between them.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Clust_intro.png](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e5/KMeans-Gaussian-data.svg/434px-KMeans-Gaussian-data.svg.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Machine learning,Clustering is an unsupervised algorithm, since there are no labels to predict.\n",
    "\n",
    "\n",
    "Why do we cluster data since we have nothing to predict?\n",
    "\n",
    "That’s to study the underlying patterns of data.\n",
    "\n",
    "Out of many clustering algorithms that we have,K-means clustering is one of the simplest \n",
    "\n",
    "and popular unsupervised machine learning algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applications of Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Few Use cases of Clustering are defined here:\n",
    "\n",
    "\n",
    "1. Market segmentation\n",
    "\n",
    "\n",
    "2. Social network analysis\n",
    "\n",
    "\n",
    "3. Search result grouping\n",
    "\n",
    "\n",
    "4. Image segmentation\n",
    "\n",
    "\n",
    "5. Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Clustering Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Centroid-based Clustering\n",
    "\n",
    "\n",
    "2. Density-based Clustering\n",
    "\n",
    "\n",
    "3. Distribution-based Clustering\n",
    "\n",
    "\n",
    "4. Hierarchical Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A important step involved in any clustering algorithm is choosing a metric based on which we are\n",
    "going to classify if any two points are similar or not.\n",
    "\n",
    "The similarity metric we are using here is Eucledian distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Eucledian distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Euclidean distance is one of most commonly used distance measurements. The figure below \n",
    "\n",
    "shows how to calculate euclidean distance between two points in a 2-dimensional space. \n",
    "\n",
    "It is calculated using Pythagoras theorum\n",
    "\n",
    "Eucledian Distance between two points (x,y) is shown below:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![distance.png](https://miro.medium.com/max/2678/0*1CRCixWSa2RTWpl4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering\n",
    "\n",
    "\n",
    "K-Means is a type of centroid based clustering algorithm.It focuses on creating K \n",
    "\n",
    "clusters of similar data points to study their underlying patterns.\n",
    "\n",
    "So, the K-means algorithm identifies k number of centroids, and then allocates every \n",
    "\n",
    "data point to the nearest cluster, while keeping the centroids as small as possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![K means.png](https://sandipanweb.files.wordpress.com/2017/03/kmeans1.gif?w=676)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How K-means run on the dataset\n",
    "\n",
    "1.Specify number of clusters K.\n",
    "\n",
    "2.Initialize centroids by randomly selecting K data points for the centroids.\n",
    "\n",
    "3.Keep iterating until no more new data points are added to clusters.\n",
    "\n",
    "4.Compute the sum of the squared distance between data points and all centroids. The less variation we have within clusters, the more homogeneous (similar) the data points are within the same cluster.\n",
    "\n",
    "5.Assign each data point to the closest cluster (centroid).\n",
    "\n",
    "6.Compute the centroids for the clusters by taking the average of the all data points that belong to each cluster.\n",
    "\n",
    "\n",
    "Unlike supervised learning where we have the ground truth to evaluate the model’s \n",
    "\n",
    "performance, clustering analysis doesn’t have a solid evaluation metric that we can use \n",
    "\n",
    "to evaluate the outcome of different clustering algorithms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![kmeans.png](https://storage.googleapis.com/aihub-c2t-containers-public/release-0.2.0/kfp-components/oob_algorithm/kmeans/assets/kmeans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing K value\n",
    "\n",
    "### Elbow Method\n",
    "\n",
    "What a good k number of clusters would be? This can be evaluated based on the sum of \n",
    "\n",
    "squared distance (SSE) between data points and their assigned clusters’ centroids. We \n",
    "\n",
    "pick k at the spot where SSE starts to flatten out and forming an elbow.\n",
    "\n",
    "In the graph shown below ,K=6 ,can be a good option!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Elbow%20curve.png](https://miro.medium.com/proxy/0*jWe7Ns_ubBpOaemM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sum of Squared Distance \n",
    "\n",
    "In the graph, the average internal per cluster is the sum of squares distance which is \n",
    "\n",
    "the Y-parameter, the number of clusters is on the X-axis.The aim is to find a visual \n",
    "\n",
    "“elbow” which is the optimal number of clusters. The average internal sum of squares is \n",
    "\n",
    "the average distance between points inside of a cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do we evaluate K-Means results?\n",
    "\n",
    "As opposed to classfication, it is difficult to assess the quality of results from \n",
    "\n",
    "clustering. Here, a metric cannot depend on the labels but only on the goodness of \n",
    "\n",
    "split. Secondly, we do not usually have true labels of the observations when we use \n",
    "\n",
    "clustering.\n",
    "\n",
    "There are internal and external goodness metrics. External metrics use the information \n",
    "\n",
    "about the known true split while internal metrics do not use any external information \n",
    "\n",
    "and assess the goodness of clusters based only on the initial data. The optimal number \n",
    "\n",
    "of clusters is usually defined with respect to some internal metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of K-Means\n",
    "\n",
    "1. Relatively simple to implement.\n",
    "\n",
    "\n",
    "2. Scales to large data sets.\n",
    "\n",
    "\n",
    "3. Guarantees convergence.\n",
    "\n",
    "\n",
    "4. Can warm-start the positions of centroids.\n",
    "\n",
    "\n",
    "5. Easily adapts to new examples.\n",
    "\n",
    "\n",
    "6. Generalizes to clusters of different shapes and sizes, such as elliptical clusters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disadvantages of K-Means\n",
    "\n",
    "\n",
    "\n",
    "1. Use the “Loss vs. Clusters” plot to find the optimal (k), as discussed in Interpret      Results.\n",
    "\n",
    "\n",
    "2. Being dependent on initial values.\n",
    "\n",
    "\n",
    "3. Clustering data of varying sizes and density.\n",
    "\n",
    "\n",
    "4. Centroids can be dragged by outliers, or outliers might get their own cluster instead    of being ignored. \n",
    "\n",
    "\n",
    "5. Scaling with number of dimensions.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Clustering\n",
    "\n",
    "\n",
    "Now, we know how K-means works we can dive into hierarchical clustering.\n",
    "\n",
    "Just as we grouped similar points in K-means, we can group clusters based on the \n",
    "\n",
    "similarity, we can combine the most similar clusters together and repeat this process \n",
    "\n",
    "until only a single cluster is left.\n",
    "\n",
    "We are essentially building a hierarchy of clusters. That’s why this algorithm is called \n",
    "\n",
    "hierarchical clustering\n",
    "\n",
    "\n",
    "Hierarchical Clusters are best visualized and explained with the help of dendograms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchichal clustering can be sub divided into two types:\n",
    "\n",
    "\n",
    "1. Agglomerative Hierarchical clustering Technique\n",
    "\n",
    "\n",
    "2. Divisive Hierarchical clustering Technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's dive  into Dendograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Dendrogram is a tree-like diagram that records the sequences of merges or splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Agglomerative Hierarchical clustering Technique: \n",
    "\n",
    "\n",
    "\n",
    "It considers each data point an individual cluster. \n",
    "\n",
    "The basic algorithm of Agglomerative is here.\n",
    "\n",
    "1. Let each data point be a cluster\n",
    "\n",
    "\n",
    "2. Merge the two closest clusters\n",
    "\n",
    "\n",
    "3. Keep merging until only a single cluster remains.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Agg_hirar_clustering.jpeg](https://miro.medium.com/max/500/1*fw1vlNtq2vPFmAXsBy1_dA.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand better let’s see a pictorial representation of the Agglomerative Hierarchical clustering Technique. \n",
    "\n",
    "\n",
    "Lets say we have six data points as shown down.\n",
    "\n",
    "1: First,we consider all the six data points as individual clusters as shown in the image below.\n",
    "\n",
    "\n",
    "2: In step two, similar clusters are merged together and formed as a single cluster. Let’s consider B,C, and D,E are similar clusters that are merged in step two. Now, we’re left with four clusters which are A, BC, DE, F.\n",
    "\n",
    "3: We again calculate the proximity of new clusters and merge the similar clusters to form new clusters A, BC, DEF.\n",
    "\n",
    "4: Calculate the proximity of the new clusters. The clusters DEF and BC are similar and merged together to form a new cluster. We’re now left with two clusters A, BCDEF.\n",
    "\n",
    "5: Finally, all the clusters are merged together and form a single cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dendrogram for above process"
   ]
  },
  {
   "attachments": {
    "Dendo.jpeg": {
     "image/jpeg": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Dendo.jpeg](https://miro.medium.com/max/500/1*JPQRbJDw2E1_HEvwzVTDDw.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divisive Hierarchical clustering Technique: \n",
    "\n",
    "We can say that the Divisive Hierarchical clustering is exactly the opposite of the Agglomerative Hierarchical clustering. \n",
    "\n",
    "In Divisive Hierarchical clustering, we consider all the data points as a single cluster \n",
    "\n",
    "and in each iteration, we separate the data points from the cluster which are not \n",
    "\n",
    "similar. Each data point which is separated is considered as an individual cluster. In \n",
    "\n",
    "the end, we’ll be left with n clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the similarity between two clusters \n",
    "\n",
    "Similarity between two clusters is important to merge or divide the clusters. There are \n",
    "\n",
    "certain approaches which are used to calculate the similarity between two clusters:\n",
    "\n",
    "1.MIN\n",
    "\n",
    "2.MAX\n",
    "\n",
    "3.Group Average\n",
    "\n",
    "4.Distance Between Centroids\n",
    "\n",
    "5.Ward’s Method\n",
    "\n",
    "Note: The one we used here is distance Between Centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Types of Linkages \n",
    "    \n",
    "    \n",
    "\n",
    "As said above during both the types of hierarchical clustering, the distance between two \n",
    "\n",
    "sub-clusters needs to be computed.  \n",
    "\n",
    "\n",
    "The different types of ways of computing the distances are:-\n",
    "\n",
    "1. Single Linkage\n",
    "\n",
    "\n",
    "2. Complete Linkage\n",
    "\n",
    "\n",
    "3. Average Linkage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Linkages.jpeg](https://www.dexlabanalytics.com/wp-content/uploads/2018/06/1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Single Linkage: For two clusters,the single linkage returns the minimum distance \n",
    "\n",
    "between two points belonging to different cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2. Complete Linkage: For two clusters , the complete linkage returns the maximum \n",
    "\n",
    "distance between two points belonging to different cluster.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3. Average Linkage: For two clusters, the arithmetic mean of distances between any data-\n",
    "\n",
    "points in one cluster to data-points in other cluster are calculated. Average Linkage \n",
    "\n",
    "returns this value of the arithmetic mean.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Hierarchical clustering Technique:\n",
    "\n",
    "There is no mathematical objective for Hierarchical clustering.\n",
    "\n",
    "All the approaches to calculate the similarity between clusters has its own disadvantages.\n",
    "\n",
    "High space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# You know Clustering now!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
