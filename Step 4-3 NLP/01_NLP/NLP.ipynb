{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Data Mining\n",
    "\n",
    "Text data mining can be described as the process of extracting essential data from common language text. All the data that we generate via text messages, documents, emails, files are written in common language text. Text mining is primarily used to draw useful insights or patterns from such data.\n",
    "\n",
    "<img src=\"https://i.imgur.com/mQvsNyf.png\" width=\"400\" height=\"350\" class=\"center\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Areas of text mining in data mining\n",
    "\n",
    "These are the following area of text mining :\n",
    "\n",
    "<br><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/Y0Ql6J3.png\" width=\"450\" height=\"450\" class=\"center\">\n",
    "\n",
    "\n",
    "<br><br>\n",
    "\n",
    "- **Information Extraction:**<br>\n",
    "The automatic extraction of structured data such as entities, entities relationships, and attributes describing entities from an unstructured source is called information extraction.\n",
    "\n",
    "- **Natural Language Processing:**<br>\n",
    "NLP stands for Natural language processing. Computer software can understand human language as same as it is spoken. NLP is primarily a component of artificial intelligence(AI). The development of the NLP application is difficult because computers generally expect humans to \"Speak\" to them in a programming language that is accurate, clear, and exceptionally structured. Human speech is usually not authentic so that it can depend on many complex variables, including slang, social context, and regional dialects.\n",
    "\n",
    "- **Data Mining:**<br>\n",
    "Data mining refers to the extraction of useful data, hidden patterns from large data sets. Data mining tools can predict behaviors and future trends that allow businesses to make a better data-driven decision. Data mining tools can be used to resolve many business problems that have traditionally been too time-consuming.\n",
    "\n",
    "- **Information Retrieval:**<br>\n",
    "Information retrieval deals with retrieving useful data from data that is stored in our systems. Alternately, as an analogy, we can view search engines that happen on websites such as e-commerce sites or any other sites as part of information retrieval.\n",
    "\n",
    "\n",
    "# Text Mining Process:\n",
    "\n",
    "The text mining process incorporates the following steps to extract the data from the document.<br><br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/sDfE2o4.png\" width=\"500\" height=\"400\">\n",
    "\n",
    "- **Text transformation**<br>\n",
    "A text transformation is a technique that is used to control the capitalization of the text.\n",
    "Here the two major way of document representation is given.\n",
    "\n",
    "    - Bag of words\n",
    "    - Vector Space\n",
    "   \n",
    "- **Text Pre-processing**<br>\n",
    "Pre-processing is a significant task and a critical step in Text Mining, Natural Language Processing (NLP), and information retrieval(IR). In the field of text mining, data pre-processing is used for extracting useful information and knowledge from unstructured text data. Information Retrieval (IR) is a matter of choosing which documents in a collection should be retrieved to fulfill the user's need.\n",
    "\n",
    "- **Feature selection**<br>\n",
    "Feature selection is a significant part of data mining. Feature selection can be defined as the process of reducing the input of processing or finding the essential information sources. The feature selection is also called variable selection.\n",
    "\n",
    "- **Data Mining**<br>\n",
    "Now, in this step, the text mining procedure merges with the conventional process. Classic Data Mining procedures are used in the structural database.\n",
    "\n",
    "- **Evaluate**<br>\n",
    "Afterward, it evaluates the results. Once the result is evaluated, the result abandon.\n",
    "\n",
    "\n",
    "- **Applications**<br>\n",
    "These are the following text mining applications:\n",
    "\n",
    "    - Risk Management\n",
    "    - Customer Care Service\n",
    "    - Business Intelligence\n",
    "    - Social Media Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "\n",
    "NLP stands for Natural Language Processing, which is a part of Computer Science, Human language, and Artificial Intelligence. It is the technology that is used by machines to understand, analyse, manipulate, and interpret human's languages. It helps developers to organize knowledge for performing tasks such as translation, automatic summarization, Named Entity Recognition (NER), speech recognition, relationship extraction, and topic segmentation.\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "There are the following applications of NLP -\n",
    "\n",
    "**1. Speech Recognition**<br>\n",
    "Speech recognition is used for converting spoken words into text. It is used in applications, such as mobile, home automation, video recovery, dictating to Microsoft Word, voice biometrics, voice user interface, and so on.<br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/EatZjYU.png\" width=\"300\" height=\"100\" class=\"center\">\n",
    "\n",
    "\n",
    "**2. Spam Detection**<br>\n",
    "Spam detection is used to detect unwanted e-mails getting to a user's inbox.<br>\n",
    "\n",
    "<img src=\"https://i.imgur.com/U31OVdW.png\" width=\"400\" class=\"center\">\n",
    "\n",
    "**3. Sentiment Analysis**<br>\n",
    "Sentiment Analysis is also known as opinion mining. It is used on the web to analyse the attitude, behaviour, and emotional state of the sender. This application is implemented through a combination of NLP (Natural Language Processing) and statistics by assigning the values to the text (positive, negative, or natural), identify the mood of the context (happy, sad, angry, etc.)\n",
    "\n",
    "<img src=\"https://i.imgur.com/AYd74N2.png\" width=\"400\" height=\"500\" class=\"center\">\n",
    "\n",
    "**4. Spelling correction**<br>\n",
    "Microsoft Corporation provides word processor software like MS-word, PowerPoint for the spelling correction.\n",
    "\n",
    "<img src=\"https://i.imgur.com/0Ir3LFQ.png\" width=\"400\" height=\"400\" class=\"center\">\n",
    "\n",
    "**5. Chatbot**<br>\n",
    "Implementing the Chatbot is one of the important applications of NLP. It is used by many companies to provide the customer's chat services.\n",
    "\n",
    "<img src=\"https://i.imgur.com/ixoPd2Z.png\" width=\"300\" height=\"400\" class=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to build an NLP pipeline\n",
    "\n",
    "There are the following steps to build an NLP pipeline -\n",
    "\n",
    "### Step1: Sentence Segmentation\n",
    "\n",
    "Sentence Segment is the first step for building the NLP pipeline. It breaks the paragraph into separate sentences.\n",
    "\n",
    "**Example:** Consider the following paragraph -\n",
    "\n",
    "**Hello everyone. Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA. Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world. You are studying NLP article.**\n",
    "\n",
    "**Sentence Segment produces the following result:**\n",
    "\n",
    "    1. \"Hello everyone. Welcome to Tech IS.Tech IS is a global programming school.\"\n",
    "    2. \"We are from Silicon Valley, USA.\"\n",
    "    3. \"Our Tutors are Professional engineers with extensive work experience in Information Technology Industry\n",
    "        hailing from around the world.\"\n",
    "    4. \"You are studying NLP article.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone.', 'Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA.', 'Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world.', 'You are studying NLP article.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/musubimanagement/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "paragraph = \"\"\"Hello everyone. Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA. \n",
    "            Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world.\n",
    "            You are studying NLP article.\"\"\"\n",
    "             \n",
    "# Tokenizing sentences\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Word Tokenization\n",
    "\n",
    "Word Tokenizer is used to break the sentence into separate words or tokens.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "**Hello everyone. Welcome to Tech IS.Tech IS is a global programming school.We are from Silicon Valley, USA. Our Tutors are Professional engineers with extensive work experience in Information Technology Industry hailing from around the world. You are studying NLP article.**\n",
    "\n",
    "**Word Tokenizer generates the following result:**\n",
    "\n",
    "'Hello', 'everyone', '.', 'Welcome', 'to', 'Tech', 'IS.Tech', 'IS', 'is', 'a', 'global', 'programming', 'school.We', 'are', 'from', 'Silicon', 'Valley', ',', 'USA', '.', 'Our', 'Tutors', 'are', 'Professional', 'engineers', 'with', 'extensive', 'work', 'experience', 'in', 'Information', 'Technology', 'Industry', 'hailing', 'from', 'around', 'the', 'world', '.', 'You', 'are', 'studying', 'NLP', 'article', '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'everyone', '.', 'Welcome', 'to', 'Tech', 'IS.Tech', 'IS', 'is', 'a', 'global', 'programming', 'school.We', 'are', 'from', 'Silicon', 'Valley', ',', 'USA', '.', 'Our', 'Tutors', 'are', 'Professional', 'engineers', 'with', 'extensive', 'work', 'experience', 'in', 'Information', 'Technology', 'Industry', 'hailing', 'from', 'around', 'the', 'world', '.', 'You', 'are', 'studying', 'NLP', 'article', '.']\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing words\n",
    "words = nltk.word_tokenize(paragraph)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Identifying Stop Words\n",
    "\n",
    "In English, there are a lot of words that appear very frequently like \"is\", \"and\", \"the\", and \"a\". NLP pipelines will flag these words as **stop words.** Stop words might be filtered out before doing any statistical analysis.\n",
    "\n",
    "**Example:** He **is a** good boy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Stemming\n",
    "\n",
    "Stemming is used to normalize words into its base form or root form. For example, celebrates, celebrated and celebrating, all these words are originated with a single root word \"celebrate.\" The big problem with stemming is that sometimes it produces the root word which may not have any meaning.\n",
    "\n",
    "**For Example,** intelligence, intelligent, and intelligently, all these words are originated with a single root word \"intelligen.\" In English, the word \"intelligen\" do not have any meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello everyon .', 'welcom tech is.tech IS global program school.w silicon valley , usa .', 'our tutor profession engin extens work experi inform technolog industri hail around world .', 'you studi nlp articl .']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Lemmatization\n",
    "\n",
    "Lemmatization is quite similar to the Stamming. It is used to group different inflected forms of the word, called Lemma. The main difference between Stemming and lemmatization is that it produces the root word, which has a meaning.\n",
    "\n",
    "**For example:** In lemmatization, the words intelligence, intelligent, and intelligently has a root word intelligent, which has a meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello everyone .', 'Welcome Tech IS.Tech IS global programming school.We Silicon Valley , USA .', 'Our Tutors Professional engineer extensive work experience Information Technology Industry hailing around world .', 'You studying NLP article .']\n"
     ]
    }
   ],
   "source": [
    "#import these modules \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) \n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BoW) model\n",
    "\n",
    "The Bag of Words (BoW) model is the simplest form of text representation in numbers. Like the term itself, we can represent a sentence as a bag of words vector (a string of numbers).\n",
    "\n",
    "Let’s recall the three types of movie reviews we saw earlier:\n",
    "\n",
    "**Review 1:** This movie is very scary and long.<br>\n",
    "**Review 2:** This movie is not scary and is slow.<br>\n",
    "**Review 3:** This movie is spooky and good.<br>\n",
    "\n",
    "We will first build a vocabulary from all the unique words in the above three reviews. The vocabulary consists of these 11 words: ‘This’, ‘movie’, ‘is’, ‘very’, ‘scary’, ‘and’, ‘long’, ‘not’,  ‘slow’, ‘spooky’,  ‘good’.\n",
    "\n",
    "We can now take each of these words and mark their occurrence in the three movie reviews above with 1s and 0s. This will give us 3 vectors for 3 reviews:\n",
    "\n",
    "<img src=\"https://i.imgur.com/Q8e9CXG.png\" width=\"500\" height=\"600\" class=\"center\">\n",
    "\n",
    "Vector of Review 1: [1 1 1 1 1 1 1 0 0 0 0]\n",
    "\n",
    "Vector of Review 2: [1 1 2 0 0 1 1 0 1 0 0]\n",
    "\n",
    "Vector of Review 3: [1 1 1 0 0 0 1 0 0 1 1]\n",
    "\n",
    "And that’s the core idea behind a Bag of Words (BoW) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 0 2 0 0 1 1 1 0 0]\n",
      " [1 0 1 0 1 1 0 1 0 1 1 0 1 0 0 0 0 0 1 1 0 0 0 1 1]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "paragrapg\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF (term frequency-inverse document frequency)\n",
    "TF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n",
    "\n",
    "## How is TF-IDF calculated?\n",
    "TF-IDF for a word in a document is calculated by multiplying two different metrics:\n",
    "\n",
    "### Term Frequency (TF)\n",
    "The number of times a word appears in a document divded by the total number of words in the document. Every document has its own term frequency.\n",
    "\n",
    "<img src=\"https://i.imgur.com/BdRVuyn.png\" width=\"200\" height=\"80\" class=\"center\">\n",
    "\n",
    "### Inverse Data Frequency (IDF)\n",
    "The log of the number of documents divided by the number of documents that contain the word w. Inverse data frequency determines the weight of rare words across all documents in the corpus.\n",
    "\n",
    "<img src=\"https://i.imgur.com/l4Qq4ZO.png\" width=\"200\" height=\"80\" class=\"center\">\n",
    "\n",
    "**Lastly, the TF-IDF is simply the TF multiplied by IDF.**\n",
    "\n",
    "<img src=\"https://i.imgur.com/RfIjTpr.png\" width=\"300\" height=\"100\" class=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.         0.70710678 0.         0.\n",
      "  0.         0.         0.70710678 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]\n",
      " [0.         0.         0.         0.         0.         0.\n",
      "  0.30151134 0.         0.         0.         0.         0.\n",
      "  0.         0.30151134 0.30151134 0.30151134 0.         0.60302269\n",
      "  0.         0.         0.30151134 0.30151134 0.30151134 0.\n",
      "  0.        ]\n",
      " [0.28867513 0.         0.28867513 0.         0.28867513 0.28867513\n",
      "  0.         0.28867513 0.         0.28867513 0.28867513 0.\n",
      "  0.28867513 0.         0.         0.         0.         0.\n",
      "  0.28867513 0.28867513 0.         0.         0.         0.28867513\n",
      "  0.28867513]\n",
      " [0.         0.57735027 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.57735027\n",
      "  0.         0.         0.         0.         0.57735027 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Cleaning the texts and creat corpus\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', sentences[i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [wordnet.lemmatize(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)\n",
    "    \n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "cv = TfidfVectorizer()\n",
    "X = cv.fit_transform(corpus).toarray()\n",
    "print(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
