# Mathematical Derivation of the Logistic regressive Model 

Logistic regression can be binary (binomial), multinomial or ordinal.

Binomial logistic regression involves an outcome with only two possible categories.

- Binomial logistic regression involves an outcome with only two possible categories.
- Multinomial logistic regression is a natural extension of the binomial logistic regression and involves an outcome with three or more possible categories.
- Ordinal is just multinomial logistic regression whereby the categories are ordered.<br>
With that said a full discussion of logistic regression cannot fit in a single Quora answer. Thus I will only focus on the simplified version of binomial logistic regression and naturally extend it to multinomial logistic regression.

We will only focus on the simplified version of binomial logistic regression and naturally extend it to multinomial logistic regression.

![img](https://user-images.githubusercontent.com/67789350/89313780-931fe180-d696-11ea-8d6e-659f54294a1e.png)

---

# Getting Started !

The outcome in binomial logistic regression can be a 0 or a 1. The idea is then to estimate the probability of an outcome being a 1 or a 0. Given that the probability of the outcome being a 1 is given by p then the probability of it not occurring is given by 1-p. This can be seen as a special case of Binomial distribution called the Bernoulli distribution.

The idea in logistic regression is to cast the problem in form of generalized linear regression model !

## ğ‘¦Ì‚ =ğ›½0+ğ›½1ğ‘¥1+â€¦+ğ›½ğ‘›ğ‘¥ğ‘›

where <span style="font-size:larger;">ğ‘¦Ì‚</span>=predicted value,  <span style="font-size:larger;"> ğ‘¥</span> = independent variables and the <span style="font-size:larger;">ğ›½</span>  are coefficients to be learnt.

This can be compactly expressed in vector form:

## ğ‘¤ğ‘‡ = [ğ›½0,ğ›½1,â€¦,ğ›½ğ‘›] 

## ğ‘¥ğ‘‡ = [1,ğ‘¥1,â€¦,ğ‘¥ğ‘›]

Then

## ğ‘¦Ì‚ = ğ‘¤áµ€ğ‘¥

But this is linear regression stuff so we need to find a way to cast the logistic regression problem in a manner whereby at least the expression above can be used. Thus if we compute the odds of the outcome as:

## ğ‘œğ‘‘ğ‘‘ğ‘ (ğ‘)= p/1-p

This now varies continuously linear and thus we can do the following:

## ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡(ğ‘)= ğ‘¦Ì‚ =ğ‘¤áµ€ğ‘¥

Thus the logit function acts like a link between logistic regression and linear regression and thus it is called a link function.

After we estimate the weights  ğ‘¤  we can take the inverse of logit to get the probability p as given below:

## ğ‘™ğ‘œğ‘”ğ‘–ğ‘¡(ğ‘)=ğ‘™ğ‘œğ‘”(ğ‘/1âˆ’ğ‘)

Taking the natural exponential on both sides gives:

## ğ‘’Ë¡áµ’áµá¶¦áµ—â½áµ–â¾=ğ‘/1âˆ’ğ‘

Then adding 1 on both sides gives:

## ğ‘’^ğ‘¦Ì‚+1=((ğ‘/1âˆ’ğ‘) + 1)

## ğ‘’^ğ‘¦Ì‚ +1= 1/1âˆ’ğ‘

Changing subject of formula to 1-p gives:

## 1âˆ’ğ‘ = 1 / ğ‘’^ğ‘¦Ì‚ + 1

Then subtracting 1 from both sides to give:

## âˆ’ğ‘ = ((1/ğ‘’^ğ‘¦Ì‚ +1) âˆ’ 1)

Multiply by -1 on both sides to get:

## ğ‘ = (1 âˆ’ (1 / ğ‘’ğ‘¦Ì‚ +1))

## ğ‘ = ğ‘’^ğ‘¦Ì‚ / ğ‘’^ğ‘¦Ì‚ + 1

Then divide the numerator and denominator by  ğ‘’^ğ‘¦Ì‚   to get:

## ğ‘ = 1 / 1+ğ‘’^âˆ’ğ‘¦Ì‚ 

Looks familiar?

_Of course it is a logistic (sigmoid) function._

From this point we can cast logistic regression in form of a single layered neural network (NN) with a sigmoid activation function. Which means we can estimate the weight parameters using the usual stochastic gradient descent (SGD) by minimizing the cross-entropy loss function given by:

## ğ‘™(ğ‘,ğ‘¦)=âˆ’ğ‘¦ğ‘™ğ‘œğ‘”(ğ‘)âˆ’(1âˆ’ğ‘¦)ğ‘™ğ‘œğ‘”(1âˆ’ğ‘)

Where  <br>
## ğ‘¦âˆˆ[0,1]

The multinomial logistic regression can then be just about adding more sigmoid neurons in the layer. And instead of using the logistic function we can use the softmax as the activation function to keep the output as a probability distribution. The loss in the multinomial logistic regression case with a softmax becomes the log likelihood loss.

---

Thus for simplicity you need to look at logistic regression from the modern formulation of standard ML techniques such as regularization and loss function optimization using gradient decent optimization methods.

Once you derive the logistic function as above, it becomes necessary to treat the rest of the logistic regression problem as just a single layered NN with sigmoid activation functions. That way we can solve the whole logistic regression problem by standard gradient descent algorithms.

## Few reference reads : 

- [Generalized linear model](https://en.m.wikipedia.org/wiki/Generalized_linear_model?wprov=sfla1)
- [Automatic differentiation](https://en.m.wikipedia.org/wiki/Automatic_differentiation?wprov=sfla1)
- [Numerical differentiation](https://en.m.wikipedia.org/wiki/Numerical_differentiation?wprov=sfla1)



 

