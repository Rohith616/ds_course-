{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k5lzR85DO3Dv"
   },
   "source": [
    "# Importance of Data Preparation \n",
    "\n",
    "- **data** refers to examples or cases from the domain that characterize the problem you want to solve\n",
    "\n",
    "- predictive modeling projects involve learning from **data**\n",
    "    - all machine learning algorithms use some **input data** to create **outputs**\n",
    "\n",
    "- this input data comprises of *features*, \n",
    "    - features which are usually in the form of columns \n",
    "\n",
    "- predictive model algorithms require features to have specific characteristics to work properly\n",
    "\n",
    "- according to a survey in Forbes, data scientists spend 60% of their time on data preparation\n",
    "\n",
    "![60% of Data Scientist's Time](https://miro.medium.com/max/700/0*-dn9U8gMVWjDahQV.jpg)\n",
    "\n",
    "- on a predictive modeling project, such as *classification* or *regression*, **raw data** typically cannot be used directly\n",
    "\n",
    "- there are four main reasons why this is the case:\n",
    "    - **data types**: machine learning algorithms require data to be numbers\n",
    "    - **data requirements**: some machine learning algorithms impose requirements on the data\n",
    "    - **data errors**: statistical noise and errors in the data may need to be corrected\n",
    "    - **data complexity**: complex nonlinear relationships may be teased out of the data\n",
    "\n",
    "- the **raw data** must be *pre-processed* prior to being used to fit and evaluate a machine learning model\n",
    "    - this step in a predictive modeling project is referred to as \"**data preparation**\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Goals \n",
    "\n",
    "- The FIFA '19  dataset can be used for several business cases \n",
    "    - build a new club\n",
    "    - choose players for awards \n",
    "    - analysis for betting on certain members\n",
    "\n",
    "- We here will be looking at building a Dream Team of 11 players \n",
    "    - we will base it on the data we have after cleaning it up \n",
    "    - we also explore some statistics concepts to come up with a Dream Team "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NFrMMLwNm6PL"
   },
   "source": [
    "# Load Python Libraries \n",
    "\n",
    "- load `numpy`\n",
    "- load `pandas`\n",
    "- load `sklearn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bWUHlTHapE5t"
   },
   "outputs": [],
   "source": [
    "# load numpy \n",
    "import numpy as np\n",
    "\n",
    "# load pandas\n",
    "import pandas as pd\n",
    "\n",
    "# configure pandas display settings\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "\n",
    "# load sklearn\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w0QEeV6Nm27Y"
   },
   "source": [
    "# Import and Review FIFA '19 Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "s08jZa4eFNhq",
    "outputId": "c532aabb-f3f7-4062-de83-05cf2e928472"
   },
   "outputs": [],
   "source": [
    "# read csv dataset from file, setting the zeroth (first) column as the index\n",
    "dataset = pd.read_csv('fifa19.csv', index_col=0)\n",
    "# set the path to your dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminary Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "swulUjrmufct"
   },
   "outputs": [],
   "source": [
    "# check dataset head\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of rows and columns (i.e. number of players and features)\n",
    "dataset.shape # (rows, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get an overview of the dataset\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Meaningless Columns\n",
    "\n",
    "- datasets usually have columns that arent meaningful inputs to create a prediction model \n",
    "\n",
    "- in our FIFA dataset, we have a few columns like that:\n",
    "    - `Photo`, `Flag` and `Club Logo` can be removed as they are simply URLs to photos \n",
    "    - `ID` column wont influence a prediction model meaningfully if used\n",
    "    - `Real Face` column also has no particular meaning \n",
    "\n",
    "- so let's drop all of those columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop meaningless columns\n",
    "dataset.drop(columns = ['Photo','Flag','Club Logo','ID', 'Real Face'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check head to confirm meaningless columns have been dropped\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Value Majority Columns\n",
    "\n",
    "- usually, 10% - 15% missing values is the cutoff for dropping a column \n",
    "    - here, we will drop all columns with more than 10% missing data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of null values for each column in the dataset \n",
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentage of missing values in each column\n",
    "round(dataset.isnull().sum()/dataset.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the columns names that have more than 10% missing values \n",
    "drop_cols = [col_name for col_name in dataset.columns if dataset[col_name].isnull().sum()/dataset.shape[0]*100 > 10.0]\n",
    "\n",
    "# list the columns to be dropped\n",
    "drop_cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns which have more than 10% missing values \n",
    "dataset.drop(columns = drop_cols, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck the percentage of mising values in each column\n",
    "round(dataset.isnull().sum()/dataset.shape[0] * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check dataset after dropping columns\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Business Goals\n",
    "\n",
    "- we've defined our business goal as coming with a dream team of 11 players\n",
    "- there can be several strategies to pick this dream team \n",
    "- we will base our selection based on some main featues \n",
    "    - We will pick a dream team based on best normalized average for:\n",
    "        - Crossing\t\n",
    "        - Finishing\t\n",
    "        - HeadingAccuracy\t\n",
    "        - ShortPassing\t\n",
    "        - Volleys\t\n",
    "        - Dribbling\t\n",
    "        - Curve\t\n",
    "        - FKAccuracy\t\n",
    "        - LongPassing\t\n",
    "        - BallControl\t\n",
    "        - Acceleration\t\n",
    "        - SprintSpeed\t\n",
    "        - Agility\t\n",
    "        - Reactions\t\n",
    "        - Balance\t\n",
    "        - ShotPower\t\n",
    "        - Jumping\t\n",
    "        - Stamina\t\n",
    "        - Strength\t\n",
    "        - LongShots\t\n",
    "        - Aggression\t\n",
    "        - Interceptions\t\n",
    "        - Positioning\t\n",
    "        - Vision\t\n",
    "        - Penalties\t\n",
    "        - Composure\t\n",
    "        - Marking\t\n",
    "        - StandingTackle\t\n",
    "        - SlidingTackle\t\n",
    "        - GKDiving\t\n",
    "        - GKHandling\t\n",
    "        - GKKicking\t\n",
    "        - GKPositioning\t\n",
    "        - GKReflexes\n",
    "    - we will then find the total price of the dream team "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Release Clause Columns\n",
    "\n",
    "- this column is a money value column, \n",
    "    - but the entries are string \n",
    "    - and are of the format \"â‚¬xxx.xM\"\n",
    "\n",
    "- this column needs to be converted to `int` from `string`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Release Clause'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Release Clause'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the release clause has to be converted to int first \n",
    "\n",
    "# define the strip and clean up function\n",
    "def str_to_int_num(rcn):\n",
    "    try:\n",
    "        return float(rcn[1:-1])*1000000\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# do the actual clean up of the Release Clause column \n",
    "dataset['Release Clause'] = dataset['Release Clause'].apply(str_to_int_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Release Clause'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Release Clause'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Relevant Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_dataset_cols = ['Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes','Release Clause']\n",
    "\n",
    "extracted_datset = dataset[extracted_dataset_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_datset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imputation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we will use the median imputation method to fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the for the number of missing values in each column\n",
    "extracted_datset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# each column's missing values is filled with the median \n",
    "for col in extracted_dataset_cols:\n",
    "    extracted_datset[col] = extracted_datset[col].fillna(extracted_datset[col].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the for the number of missing values in each column\n",
    "extracted_datset.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- for some learning algorithms, for the input features \n",
    "    - bigger numbers influence the learning model more\n",
    "    - smaller numbers influence the learning model less \n",
    "\n",
    "- this is because the inputs have different ranges \n",
    "    - to avoid this range effect on the prediciton algorithm, all inputs are scaled to a comparable values \n",
    "\n",
    "- normalization and standardization are two common methods of scaling input data \n",
    "\n",
    "- we shall use a `MinMaxScalar` to normalize the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extracted_datset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- seperate out the input features (X) and the target label (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is the set of input features \n",
    "X = extracted_datset.drop(['Release Clause'], axis=1)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y is the label, in this case, the Release Clause column\n",
    "y = extracted_datset['Release Clause']\n",
    "y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale only the input features \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the MinMax Scaler from sklearn \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# initialize the min-max-scaler \n",
    "min_max_scaler = MinMaxScaler()\n",
    "\n",
    "# fit X to the scaler \n",
    "min_max_scaler.fit(X)\n",
    "\n",
    "# perform the actual scaling on X \n",
    "extracted_datset_scaled = pd.DataFrame(min_max_scaler.transform(X))\n",
    "\n",
    "# check preliminary stats to verify that scaling was successfully applied\n",
    "extracted_datset_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the extracted, scaled datset\n",
    "extracted_datset_scaled.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names of scaled features\n",
    "scaled_model_names = ['Crossing', 'Finishing', 'HeadingAccuracy', 'ShortPassing', 'Volleys', 'Dribbling', 'Curve', 'FKAccuracy', 'LongPassing', 'BallControl', 'Acceleration', 'SprintSpeed', 'Agility', 'Reactions', 'Balance', 'ShotPower', 'Jumping', 'Stamina', 'Strength', 'LongShots', 'Aggression', 'Interceptions', 'Positioning', 'Vision', 'Penalties', 'Composure', 'Marking', 'StandingTackle', 'SlidingTackle', 'GKDiving', 'GKHandling', 'GKKicking', 'GKPositioning', 'GKReflexes']\n",
    "\n",
    "# assign appropriate column names to extract\n",
    "extracted_datset_scaled.columns = scaled_model_names\n",
    "\n",
    "# read the head of the extracted dataset after the column labels have been applied\n",
    "extracted_datset_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assemble Data for Business Goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe from the average of the Feature Set, Names of Players, and Release Clause\n",
    "three_set = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capture the names of the players form the original dataset\n",
    "three_set['Name'] = dataset['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the average score for each player\n",
    "three_set['Average Score'] = extracted_datset_scaled.sum(axis=1)/len(scaled_model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the dataframe currently\n",
    "three_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the release clause for each person \n",
    "three_set['Release Clause'] = extracted_datset['Release Clause']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head of the assembled dataframe\n",
    "three_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort the dataframe by average score in descending order\n",
    "three_set.sort_values(by=['Average Score'], ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the top 11 players with the best average scores \n",
    "three_set.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "- compute the total Release Clasue for this Dream Team of 11\n",
    "- compute the average wage that will be paid to the dream team\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing DateTime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- when working with date-time components in datasets, it is important to sanitize them to a common format\n",
    "- here in our FIFA dataset, we have a couple data-time columns, lets examine and sanitize them as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joined Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the datatype of 'Joined'\n",
    "dataset['Joined'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head after clean up\n",
    "dataset['Joined'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check value counts of 'Joined'\n",
    "# dataset['Joined'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import the time \n",
    "import datetime\n",
    "\n",
    "# define the datetime convertor function\n",
    "def data_str_to_datetime_1(date_str):\n",
    "    try:\n",
    "        return datetime.datetime.strptime(date_str, '%b %d, %Y')\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# do the actual clean up of the Joined column \n",
    "dataset['Joined'] = dataset['Joined'].apply(data_str_to_datetime_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the head after clean up\n",
    "dataset['Joined'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contract Valid Until"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the datatype of 'Contract Valid Until'\n",
    "dataset['Contract Valid Until'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique value counts in 'Contract Valid Until'\n",
    "dataset['Contract Valid Until'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define the datetime cleanup function\n",
    "def data_str_to_datetime_2(date_str):\n",
    "\n",
    "    try:\n",
    "        if date_str.find(',') != -1:\n",
    "            curr_date = datetime.datetime.strptime(date_str, '%b %d, %Y')\n",
    "            return curr_date.strftime(\"%Y\")\n",
    "        elif date_str.find(',') == -1:\n",
    "            curr_date = datetime.datetime.strptime(date_str, '%Y')\n",
    "            return curr_date.strftime(\"%Y\")\n",
    "\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "# do the actual clean up of the Joined column \n",
    "dataset['Contract Valid Until'] = dataset['Contract Valid Until'].apply(data_str_to_datetime_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the datatype of 'Contract Valid Until'\n",
    "dataset['Contract Valid Until'].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check unique value counts in 'Contract Valid Until'\n",
    "dataset['Contract Valid Until'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorical Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- categorical data can be converted to numerical data using encoding \n",
    "- this makes it possbile to create a numerical input even for categorical, string-like features\n",
    "- there are two popular kinds of encoding categorical data \n",
    "    - Label Encoding: \n",
    "        - assigns a numerical value to a particular value of the categorical variable\n",
    "    - One-Hot Encoding: \n",
    "        - creates new columns for each possible value of the categorical variable, \n",
    "        - uses binary value to classify presence in the original feature columns\n",
    "\n",
    "- [Relevant Reading - Choosing the right Encoding method-Label vs OneHot Encoder](https://towardsdatascience.com/choosing-the-right-encoding-method-label-vs-onehot-encoder-a4434493149b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Club` and `Position` are both categorial variables\n",
    "- let's explore Label Encoding for `Club` and One-Hot Encoding for `Position`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label Encoding \n",
    "\n",
    "- we will apply Label Encoding for `Club`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many nas exist in the Club column\n",
    "dataset.Club.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing CLub values with Unknown\n",
    "dataset.Club.fillna('Unknown',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how many nas exist in the Club column\n",
    "dataset.Club.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import preprocessing library from sklearn\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# initialize a label encoder \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "# fit the data to the label encoder \n",
    "label_encoder.fit(dataset['Club'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transfrom the column and save it back into the main DataFrame\n",
    "dataset['Club'] = label_encoder.transform(dataset['Club'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chcke the label encoded Club column in the main DataFrame \n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding \n",
    "\n",
    "- we will apply One Hot encoding for `Position`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of unique values in Position\n",
    "dataset['Position'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check number of null values \n",
    "dataset['Position'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace null values with 'Unknown'\n",
    "dataset['Position'].fillna('Unknown', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recheck number of null values \n",
    "dataset['Position'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of unique values in Position\n",
    "len(dataset['Position'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so 28 new columns will be added as a result on one-hot encoding\n",
    "- but we will remove the original column after one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the current shape of dataframe \n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dummy variables \n",
    "encoded_columns = pd.get_dummies(dataset.Position)\n",
    "\n",
    "encoded_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the data set with the encoded columns\n",
    "dataset = dataset.join(encoded_columns).drop('Position',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the first five rows of the dataframe\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of one-hot encoded data frame\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- one of the very common issues while developing Machine Learning systems is *overfitting*\n",
    "\n",
    "- to avoid this to a large extent, the available data is split into two parts\n",
    "    - a training part \n",
    "    - a test/validation part \n",
    "\n",
    "- the training part is used to fit the actual model \n",
    "- the testing/validation part is used to provide an unbiased evaluation of a model fit on the training dataset \n",
    "\n",
    "- the model never learns from the testing/validation part\n",
    "\n",
    "- below is a demonstration of doing a train-test split for a dataset\n",
    "    - this if often done at the very end of the clean up process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# do a 80% train - 20% test split\n",
    "X_train, X_test, y_train, y_test = train_test_split( X , y , test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check X_train shape \n",
    "X_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check X_test shape \n",
    "X_test.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check X_train shape \n",
    "y_train.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check X_test shape \n",
    "y_test.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Data_Pre_Processing_Techniques_FIFA_19_Dataset.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
