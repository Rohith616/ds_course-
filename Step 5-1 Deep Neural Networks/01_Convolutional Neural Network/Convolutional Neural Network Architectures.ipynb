{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of a CNN is a key factor in determining its performance and efficiency. The way in which the layers are structured, which elements are used in each layer and how they are designed will often affect the speed and accuracy with which it can perform various tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Popular architectures - the differences in architecture and their advantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many popular CNN architectures, many of them gained recognition by achieving good results at the ILSVRC (ImageNet Large Scale Visual Recognition Challenge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet-5\n",
    "\n",
    "Yann Lecun's LeNet-5 model was developed in 1998 to identify handwritten digits for zip code recognition in the postal service. This pioneering model largely introduced the convolutional neural network as we know it today.\n",
    "<br>\n",
    "#### Architecture\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/CJG23qy.png\" width=\"970\" hight = \"800\" > <br>\n",
    "\n",
    "It takes a grayscale image as input. Once we pass it through a combination of convolution and pooling layers, the output will be passed through fully connected layers and classified into corresponding classes. The total number of parameters in LeNet-5 are:\n",
    "\n",
    "- **Parameters:** 60k <br>\n",
    "- **Layers flow:** Conv -> Pool -> Conv -> Pool -> FC -> FC -> Output<br>\n",
    "- **Activation functions:** Sigmoid/tanh and ReLu<br>\n",
    "<br><br>\n",
    "\n",
    "### AlexNet\n",
    "\n",
    "AlexNet was developed by Alex Krizhevsky et al. in 2012 to compete in the ImageNet competition. The general architecture is quite similar to LeNet-5, although this model is considerably larger. The success of this model (which took first place in the 2012 ImageNet competition) convinced a lot of the computer vision community to take a serious look at deep learning for computer vision tasks.\n",
    "\n",
    "#### Architecture\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/WXjweaT.png\" width=\"970\" hight = \"800\" > <br>\n",
    "\n",
    "- **Parameters:** 60 million\n",
    "- **Activation function:** ReLu\n",
    "\n",
    "<br><br><br>\n",
    "\n",
    "### VGGNet\n",
    "\n",
    "The runner-up in ILSVRC 2014 was the network from Karen Simonyan and Andrew Zisserman that became known as the VGGNet. Its main contribution was in showing that the depth of the network is a critical component for good performance. Their final best network contains 16 CONV/FC layers and, appealingly, features an extremely homogeneous architecture that only performs 3x3 convolutions and 2x2 pooling from the beginning to the end. Their pretrained model is available for plug and play use in Caffe. \n",
    "\n",
    "A downside of the VGGNet is that it is more expensive to evaluate and uses a lot more memory and parameters (140M). Most of these parameters are in the first fully connected layer, and it was since found that these FC layers can be removed with no performance downgrade, significantly reducing the number of necessary parameters.\n",
    "\n",
    "#### Architecture\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/dxn9wvh.png\" width=\"600\" > <br><br>\n",
    "\n",
    "- **Parameters:** 138 million\n",
    "<br><br><br>\n",
    "\n",
    "\n",
    "### GoogleNet (2014)\n",
    "\n",
    "Built with a CNN inspired by LetNet, the GoogleNet network, which is also named Inception V1, was made by a team at Google. GoogleNet was the winner of ILSVRC 2014 and achieved a top-5 error rate of less than 7%, which is close to the level of human performance.\n",
    "\n",
    "GoogleNet architecture consisted of a 22 layer deep CNN used a module based on small convolutions, called “inception module”, which used batch normalization, RMSprop and image to reduce the number of parameters from 60 million like in AlexNet to only 4 million.\n",
    "\n",
    "#### Architecture\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/BMHWY15.png\"> <br>\n",
    "<br>\n",
    "\n",
    "**The alternate view of this architecture in a tabular format in below:**\n",
    "\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/2yueDek.png\"> <br>\n",
    "<br>\n",
    "<br><br>\n",
    "\n",
    "### ResNet (2015)\n",
    "\n",
    "The winner of ILSRVC 2015, it also called as Residual Neural Network (ResNet) by Kaiming. This architecture introduced a concept called “skip connections”. Typically, the input matrix calculates in two linear transformation with ReLU activation function. In Residual network, it directly copy the input matrix to the second transformation output and sum the output in final ReLU function.\n",
    "\n",
    "#### Architecture\n",
    "<br>\n",
    "<img src=\"https://i.imgur.com/3q50xFI.png\" width=\"970\" hight = \"800\" > <br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
