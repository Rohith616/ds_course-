{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transfer_learning_introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgqsKfc4D8zT"
      },
      "source": [
        "![img](https://user-images.githubusercontent.com/67789350/100412060-7ec68100-3099-11eb-933f-019dc954cc10.png)\n",
        "# Introduction\n",
        "Transfer learning consists of taking features learned on one problem, and leveraging them on a new, similar problem. For instance, features from a model that has learned to identify racoons may be useful to kick-start a model meant to identify Tanukis.\n",
        "\n",
        "Transfer learning is usually done for tasks where your dataset has too little data to train a full-scale model from scratch.\n",
        "\n",
        "The most common incarnation of transfer learning in the context of deep learning is the following worfklow:\n",
        "\n",
        "- Take layers from a previously trained model.\n",
        "- Freeze them, so as to avoid destroying any of the information they contain during future training rounds.\n",
        "- Add some new, trainable layers on top of the frozen layers. They will learn to turn the old features into predictions on a new dataset.\n",
        "- Train the new layers on your dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gf8-E4jUERam"
      },
      "source": [
        "# Understanding Transfer Learning\n",
        "\n",
        "The first thing to remember here is that, transfer learning, is not a new concept which is very specific to deep learning. There is a stark difference between the traditional approach of building and training machine learning models, and using a methodology following transfer learning principles.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100411334-7a996400-3097-11eb-860c-b6313d3c48f7.png)\n",
        "\n",
        "Traditional learning is isolated and occurs purely based on specific tasks, datasets and training separate isolated models on them. No knowledge is retained which can be transferred from one model to another. In transfer learning, you can leverage knowledge (features, weights etc) from previously trained models for training newer models and even tackle problems like having less data for the newer task!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CIh5D29EUdn"
      },
      "source": [
        "# Key Considerations\n",
        "\n",
        "Transfer learning, as we have seen so far, is having the ability to utilize existing knowledge from the source learner in the target task. During the process of transfer learning, the following three important questions must be answered:\n",
        "\n",
        "## What to transfer: \n",
        "\n",
        "This is the first and the most important step in the whole process. We try to seek answers about which part of the knowledge can be transferred from the source to the target in order to improve the performance of the target task. When trying to answer this question, we try to identify which portion of knowledge is source-specific and what is common between the source and the target.\n",
        "\n",
        "## When to transfer: \n",
        "\n",
        "There can be scenarios where transferring knowledge for the sake of it may make matters worse than improving anything (also known as negative transfer). We should aim at utilizing transfer learning to improve target task performance/results and not degrade them. We need to be careful about when to transfer and when not to.\n",
        "\n",
        "## How to transfer: \n",
        "\n",
        "Once the what and when have been answered, we can proceed towards identifying ways of actually transferring the knowledge across domains/tasks. This involves changes to existing algorithms and different techniques, which we will cover in later sections of this article. Also, specific case studies are lined up in the end for a better understanding of how to transfer.\n",
        "\n",
        "This should help us define the various scenarios where transfer learning can be applied and possible techniques."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t65naZkzEafp"
      },
      "source": [
        "# Transfer Learning Strategies\n",
        "\n",
        "There are different transfer learning strategies and techniques, which can be applied based on the domain, task at hand, and the availability of data\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100411788-ba148000-3098-11eb-9614-3c4ae4728c62.png)\n",
        "\n",
        "From the above picture that gives an overview about possible different stratergies :\n",
        "\n",
        "## Inductive Transfer learning:\n",
        "\n",
        "In this scenario, the source and target domains are the same, yet the source and target tasks are different from each other. The algorithms try to utilize the inductive biases of the source domain to help improve the target task. Depending upon whether the source domain contains labeled data or not, this can be further divided into two subcategories, similar to multitask learning and self-taught learning, respectively.\n",
        "\n",
        "## Unsupervised Transfer Learning:\n",
        "\n",
        "This setting is similar to inductive transfer itself, with a focus on unsupervised tasks in the target domain. The source and target domains are similar, but the tasks are different. In this scenario, labeled data is unavailable in either of the domains.\n",
        "\n",
        "## Transductive Transfer Learning: \n",
        "\n",
        "In this scenario, there are similarities between the source and target tasks, but the corresponding domains are different. In this setting, the source domain has a lot of labeled data, while the target domain has none. This can be further classified into subcategories, referring to settings where either the feature spaces are different or the marginal probabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1NVDNaEcCR"
      },
      "source": [
        "# Formal Definition\n",
        "\n",
        "Let’s now take a look at a formal definition for transfer learning and then utilize it to understand different strategies. In their paper, [A Survey on Transfer Learning](https://www.cse.ust.hk/~qyang/Docs/2009/tkde_transfer_learning.pdf), Pan and Yang use domain, task, and marginal probabilities to present a framework for understanding transfer learning. \n",
        "\n",
        "The **framework** is defined as follows:\n",
        "\n",
        "A **domain**, D, is defined as a two-element tuple consisting of feature space, \n",
        "\n",
        "and **marginal probability**, P(Χ), where Χ is a sample data point. \n",
        "\n",
        "Thus, we can represent the **domain** mathematically as D = { , P(Χ)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPL-Px4SErR6"
      },
      "source": [
        "# Getting Started\n",
        "\n",
        "Now that you are armed with the techinical aspects of performing a meaningful transfer learning \n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100414785-905f5700-30a0-11eb-9290-7fd8e55cf4ea.png)\n",
        "\n",
        "For better intution of how exactly to use let's discuss a theorotical example of two problems \n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100415209-a1f52e80-30a1-11eb-9a78-9b144c4d8b81.png)\n",
        "\n",
        "We will train our model on the first problem statement and save it's weights \n",
        "\n",
        "And then we will use these weights from the process of the first problem statement and use them while training the second problem statement \n",
        "\n",
        "Thus by using the weights of the first problem to solve the second one , we are transfering the knowldge that we learned from the first problem statement and using it to solve the second one !\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100416008-6e1b0880-30a3-11eb-82eb-e620ce572ba1.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STs11QfZRLOg"
      },
      "source": [
        "# How is learning stored ?\n",
        "\n",
        "Weights are the basic building blocks of neural networks. A weight consists of a tensor-in tensor-out computation function (the layer's call method) and some state, held in TensorFlow variables (the layer's weights).\n",
        "\n",
        "When a neural network is trained on the training set, it is initialised with a set of weights. These weights are then optimised during the training period and the optimum weights are produced.\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100416284-18932b80-30a4-11eb-8a48-5c29027c3bfb.png)\n",
        "\n",
        "Weights are the co-efficients of the equation which you are trying to resolve. Negative weights reduce the value of an output.\n",
        "\n",
        "# Let's use an example! \n",
        "\n",
        "Assume you are predicting the price of a car in dollars. Your understanding is that the price of the car is dependent on the year it was made and the number of miles it has driven.\n",
        "\n",
        "Let’s assume that your hypothesis is that the higher the year of the car, the pricey the car. And subsequently, the more the car is driven, the cheaper the car.\n",
        "\n",
        "The weights are essentially reflecting how important an input is.\n",
        "This example should help you see that there is a positive relationship between the price of the car and the year it was made and a negative relationship between the price of the car and the miles it has been driven. As a result, we expect to see positive weight for the feature that represents year and negative weight for the feature that represents miles."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8hamM18U2F0"
      },
      "source": [
        "# How is learning used by a different Model ?\n",
        "\n",
        "After the training process is concluded on first model , those weights can be saved in some format (usually H5P file) \n",
        "\n",
        "And while training on the second problem you can just use these pre-trained weights and use them while classifying or segmenting your second problem !\n",
        "\n",
        "![img](https://user-images.githubusercontent.com/67789350/100417045-f6021200-30a5-11eb-95e9-6a668d595816.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFbbYA0nWEuD"
      },
      "source": [
        "# Endnotes\n",
        "\n",
        "you discovered how to use transfer learning when developing convolutional neural networks for computer vision applications.\n",
        "\n",
        "Specifically, you learned:\n",
        "\n",
        "- Transfer learning involves using models trained on one problem as a starting point on a related problem.\n",
        "- Transfer learning is flexible, allowing the use of pre-trained models directly as feature extraction preprocessing and integrated into entirely new models.\n",
        "\n",
        "We will try to implement these logics and the transfer learning concepts on a problem statement!\n",
        "\n",
        "Head on over to the notebook accompanying these academic material , happy coding !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSWSi-KEWwGt"
      },
      "source": [
        "# Supplimentary Resources \n",
        "\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Transfer_learning)\n",
        "- [Official Nvidia Blog](https://blogs.nvidia.com/blog/2019/02/07/what-is-transfer-learning/)\n",
        "- [Andrew Ng on Transfer Learing](https://www.youtube.com/watch?v=yofjFQddwHE&ab_channel=Deeplearning.ai)"
      ]
    }
  ]
}
